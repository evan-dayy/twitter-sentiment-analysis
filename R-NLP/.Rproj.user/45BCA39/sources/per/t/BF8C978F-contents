---
title: "Sentiment Analysis"
author: "Dai Yichao (IVAN)"
date: "4/15/2021"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

---

```{r, include=FALSE, warning=FALSE, message=FALSE}
library(readxl)
library(dplyr)
library(tidytext)
library(tm)
library(ggplot2)
library(stringr)
library(tidytext)
library(textdata)
library(tidyr)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(caret)
```

---

## AAPL

---- 

### Read Text file and Text Cleanning 

```{r,warning=FALSE, message=FALSE}
# AAPL ---------------------
# read data------------------
AAPL = read.csv('AAPL 03-26 to 04-02 & 04-04 to 04-09.csv')

# time with one-hour gap -------------
AAPL = AAPL[,c(3,5)]
AAPL$created_at = as.character(AAPL$created_at)
AAPL$created_at = strptime(gsub('T', " ",
                                substr(AAPL$created_at,1,13)), 
                           format = "%Y-%m-%d %H") # time gap with hour
text_df = tibble(time = AAPL$created_at,text = AAPL$text)
text_df$text = as.character(text_df$text)
text_df = text_df %>%
        mutate(date = as.Date(time))

# remove the ... ---------------------

Textprocessing <- function(x)
{       x = gsub("?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", x)
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub("(?:\\s*#\\w+)+\\s*$", " ", x)
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', ' ', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', ' ', x) ## Remove Punctuations
x = gsub("^[[:space:]]*"," ",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$"," ",x) ## Remove trailing whitespaces
x = gsub('[0-9]+', ' ', x) ## Remove all the number 
gsub(' +',' ',x) ## Remove extra whitespaces
}

text_df$text = Textprocessing(text_df$text)
```

The following table shows the tweet number per hour with a barplot. 

```{r,warning=FALSE, message=FALSE}
# create the byhour-text_df -----------------
text_df$time = as.character(text_df$time)
text_df1= text_df %>%
        group_by(time) %>%
        count(time) %>%
        arrange(time)
text_df %>%
        group_by(time) %>%
        count(time) %>%
        arrange(time) %>%
        ggplot(aes(x = as.POSIXct(time), y = n, fill = as.character(time))) +
        geom_col(show.legend = FALSE)+theme_bw()
library(writexl)
write_xlsx(text_df1, 'aapl.xlsx')

```

paste all the text together group by hour, the following table shows an example of the text dataframe. 

```{r,warning=FALSE, message=FALSE}
byhour_text_df = text_df %>% 
        group_by(date,time) %>%
        summarise_all(paste, collapse = ' ') # paste the test together group by 1 hour
head(byhour_text_df)
paste('there are total', nrow(byhour_text_df), 'observation')
```


```{r, warning=FALSE, message=FALSE}
# Tidy the text -------
tidy_books <- text_df %>%
        unnest_tokens(word, text)%>% # separate words 
        anti_join(stop_words) ## omit the stop words 
```

---

### Sentiment Data frame with bing, afinn, and nrc

We start with the bing data frame

```{r,warning=FALSE, message=FALSE}

# sentiment analysis --------
## Bing method
bing<- get_sentiments("bing")
sentment_bing = tidy_books %>%
        inner_join(bing) %>%
        count(date, time, word, sentiment) %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>% 
        group_by(date, time) %>%
        summarise(sentiment = sum(sentiment))
head(sentment_bing)
```

then, we normalize the sentiment, normalized data has mean = 0 // aother way is rescale to c(-3,3)


```{r,warning=FALSE, message=FALSE}
library(scales)
normalize = function(x){
        (x-mean(x))/sd(x)
}
sentment_bing$sentiment = normalize(sentment_bing$sentiment)
head(sentment_bing)
```

and then, we plot the normalized sentiment against the time.

```{r,warning=FALSE, message=FALSE}
ggplot(sentment_bing, aes(as.POSIXct(time), sentiment, fill = date)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(.~date, ncol = 3, scales = "free_x")+
        theme_bw()+labs(title = 'BING', x = 'Date')
p1 = ggplot(sentment_bing, aes(as.POSIXct(time), sentiment, fill = as.character(date))) +
        geom_col(show.legend = FALSE)+
        theme_bw()+labs(title = 'BING', x = 'Date')+theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
        scale_x_datetime(breaks = date_breaks('1 day'), labels = date_format("%m-%d"))
p1
```

And then, we deal with the afinn sentiment dataframe


```{r,warning=FALSE, message=FALSE}
## afinn
afinn<- get_sentiments("afinn")
sentment_afinn = tidy_books %>%
        inner_join(afinn) %>%
        group_by(date, time) %>%
        summarise(sentiment = sum(value))

sentment_afinn$sentiment = normalize(sentment_afinn$sentiment)
head(sentment_afinn)
```

and then, we plot the normalized sentiment against the time. // Aother method is rescale to c(-3,3)

```{r,warning=FALSE, message=FALSE}
ggplot(sentment_afinn, aes(as.POSIXct(time), sentiment, fill = date)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(.~date, ncol = 3, scales = "free_x")+
        theme_bw()+
        labs(title = 'AFINN', x = 'Date')

p2 = ggplot(sentment_afinn, aes(as.POSIXct(time), sentiment, fill = as.character(date))) +
        geom_col(show.legend = FALSE)+
        theme_bw()+
        labs(title = 'AFINN', x = 'Date')+theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
        scale_x_datetime(breaks = date_breaks('1 day'), labels = date_format("%m-%d"))
p2
```

we compare the two sentiment plot together

```{r,warning=FALSE, message=FALSE}
grid.arrange(p1, p2, ncol = 1)
```

using t-test to check the whether there is a difference between bing lexicon and afinn lexicon, however the distriibution must be similar. ( this is meaningless, because we have already normalize the data, the distributio will be almost the same

```{r}
library(statsr)
bing_afinn = rbind(sentment_bing, sentment_afinn)
bing_afinn  = cbind(bing_afinn, method = rep(c('bing', 'afinn'), each = 301))
inference(y = sentiment, x = method, data = bing_afinn, type = 'ht',
          method = 'theoretical', statistic = 'mean', alternative = 'twosided')

```

we should use the KS-test to check the distribution: as a result, reject the null h0, the distribution are different. 

```{r}
bing_afinn = left_join(sentment_bing, sentment_afinn, by = 'time')
colnames(bing_afinn)[c(3,5)] = c('bing', 'afinn')
ks.test(bing_afinn$bing,bing_afinn$afinn, alternative = 'two.sided')
```

Then, here is the method with nrc lexicon

```{r,warning=FALSE, message=FALSE}
## nrc
nrc<- get_sentiments("nrc")
sentment_nrc = tidy_books %>%
        inner_join(nrc) %>%
        count(date, time, word, sentiment) %>%
        spread(sentiment, n, fill = 0) %>%
        select(-word)%>%
        group_by(date, time) %>%
        summarise_all(sum)


```

```{r}
library(reshape2)
melt_nrc = cbind(time =rep(sentment_nrc$time,10),  melt(sentment_nrc[,3:12]))
ggplot(melt_nrc, aes(as.POSIXct(time), value, fill = variable)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(.~variable, ncol = 2, scales = "free_x")+
        theme_bw()+labs(title = 'BING', x = 'Date')+
        scale_x_datetime(breaks = date_breaks('2 days'), labels = date_format("%m-%d"))

```


---


### top 10 bing_words Count


```{r}
bing_word_counts <- tidy_books %>%
        inner_join(get_sentiments("bing")) %>%
        count(word, sentiment, sort = TRUE) %>%
        ungroup()

bing_word_counts %>%
        group_by(sentiment) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(word = reorder(word, n)) %>%
        ggplot(aes(n, word, fill = sentiment)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~sentiment, scales = "free_y") +
        labs(x = "Contribution to sentiment",
             y = NULL)+
        theme_bw()
```

### top 10 nrc_words Count

```{r, fig.width=8}
nrc_word_counts <- tidy_books %>%
        inner_join(get_sentiments("nrc")) %>%
        count(word, sentiment, sort = TRUE) %>%
        ungroup()

nrc_word_counts %>%
        group_by(sentiment) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(word = reorder(word, n)) %>%
        ggplot(aes(n, word, fill = sentiment)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~sentiment, scales = "free_y", ncol = 5) +
        labs(x = "Sentiment Count",
             y = NULL)+
        theme_bw()
```


---

### AAPL
### Stock Information

```{r}
stock_aapl = read_excel('AAPL GOOG....xlsx', sheet = 'aapl') # read the stock Information 
stock_aapl = stock_aapl[,c(3,2)]
stock_aapl$time = as.character(stock_aapl$time )
stock_aapl$time = strptime(gsub('T', " ",
                                substr(stock_aapl$time,1,13)), 
                           format = "%Y-%m-%d %H") # time gap with hour
stock_aapl$time = as.character(stock_aapl$time )
stock = stock_aapl %>% arrange(time)
head(stock)
```

normalize the price data:

```{r}
stock$price = normalize(stock$price)
head(stock)
```


```{r}

stock_afinn = ggplot() +
        geom_col(show.legend = FALSE, 
                 data =sentment_afinn, 
                 aes(as.POSIXct(time), sentiment, fill = as.character(date)))+
        geom_line(show.legend = FALSE, data = stock, 
                  aes(x = as.POSIXct(time), 
                      price, group = 1, alpha = 0.6))+
        geom_point(show.legend = FALSE, data = stock, aes(x = as.POSIXct(time),
                                                         price,group = 1),
                  alpha = 0.1)+
        theme_bw()+
        labs(title = 'AFINN', x = 'Date')+theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
        scale_x_datetime(breaks = date_breaks('1 day'), labels = date_format("%m-%d"))
     
stock_bing = ggplot() +
        geom_col(data = sentment_bing, aes(as.POSIXct(time), sentiment, fill = as.character(date)),
                 show.legend = FALSE)+
        geom_line(show.legend = FALSE, data = stock, aes(x = as.POSIXct(time),
                                                         price,group = 1),
                  alpha = 0.6)+
        geom_point(show.legend = FALSE, data = stock, aes(x = as.POSIXct(time),
                                                         price,group = 1),
                  alpha = 0.1)+
        theme_bw()+labs(title = 'BING', x = 'Date')+theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
        scale_x_datetime(breaks = date_breaks('1 day'), labels = date_format("%m-%d"))

grid.arrange(stock_afinn, stock_bing, ncol = 1)
```

2. Build the model dataframe:

```{r}
## build the data frame ----------
## for nrc:
colnames(stock)[1] = 'datetime'
stock$date = as.Date(stock$datetime, format = "%Y-%m-%d")
stock$datetime = as.POSIXct(stock$datetime)
stock$time_stock = format(stock$datetime, format = '%H:%M')
stock = stock[order(stock$datetime, decreasing = FALSE),]

colnames(sentment_nrc)[2] = 'datetime'
sentment_nrc$datetime = as.POSIXct(sentment_nrc$datetime)
sentment_nrc$time_se = format(sentment_nrc$datetime, format = '%H:%M')
full_nrc = full_join(stock,sentment_nrc)
```

Here we need to deal with several questions:
        1. Stock maket open at 9 am and close at 4 pm
        2. At the open time, stock market record the XX:30, which is not consistent with sentimennt XX::00
        3. At close time, stock market also record some stock price


```{r}
# figure out whether it is close or open time
full_nrc$time_stock = ifelse(is.na(full_nrc$time_stock), 
                             full_nrc$time_se, full_nrc$time_stock) 
full_nrc = full_nrc[,-15]
opentime = "09:00"
closetime = "16:00"
full_nrc$state = ifelse((strptime(full_nrc$time_stock, format = '%H:%M') >= strptime(opentime, format = '%H:%M')) & (strptime(full_nrc$time_stock, format = '%H:%M') <= strptime(closetime, format = '%H:%M')), 'open', 'close' )
appl_nrc = write_xlsx(full_nrc, 'appl_nrc.xlsx')

```

Separate the dataframe into close data_frame and open data_frame

```{r}
full_nrc_close =  full_nrc%>%
        filter(state == 'close')
full_nrc_close = na.omit(full_nrc_close)
head(full_nrc_close)
full_nrc_open = full_nrc %>%
        filter(state == 'open')
full_nrc_open = na.omit(full_nrc_open)
head(full_nrc_open)

full_nrc = rbind(full_nrc_close,full_nrc_open)

full_nrc$anger  = normalize(full_nrc$anger )
full_nrc$anticipation  = normalize(full_nrc$anticipation )
full_nrc$disgust  = normalize(full_nrc$disgust )
full_nrc$fear  = normalize(full_nrc$fear )
full_nrc$joy  = normalize(full_nrc$joy )
full_nrc$negative  = normalize(full_nrc$negative )
full_nrc$positive  = normalize(full_nrc$positive )
full_nrc$sadness  = normalize(full_nrc$sadness )
full_nrc$surprise  = normalize(full_nrc$surprise )
full_nrc$trust  = normalize(full_nrc$trust )

```

---

### AAPL NRC Regression Model result

1. this is the model for total recording
```{r}
md_nrc = lm(price ~ anger+anticipation+disgust+fear+joy+negative+positive+sadness+surprise+trust, data = full_nrc)
summary(md_nrc)
```

2. this is the model for close recording

```{r}
md_nrc_close = lm(price ~ anger+anticipation+disgust+fear+joy+negative+positive+sadness+surprise+trust, data = full_nrc[which(full_nrc$state == 'close'), ])
summary(md_nrc_close)
```

3. this is the model for open recording

```{r}
md_nrc_open = lm(price ~ anger+anticipation+disgust+fear+joy+negative+positive+sadness+surprise+trust, data = full_nrc[which(full_nrc$state == 'open'), ])
summary(md_nrc_open)
```


the most relative variable is the trust sentiment, plotting its plot and stock price

```{r}
ggplot(data = full_nrc) +
        geom_col(aes(as.POSIXct(datetime), trust, fill = state))+
        geom_line(aes(x = as.POSIXct(datetime),price,group = 1),
                  alpha = 0.6)+
        geom_point(aes(x = as.POSIXct(datetime),price,group = 1, col = 2),
                  alpha = 0.2, show.legend = FALSE)+
        theme_bw()+labs(title = 'BING', x = 'Date')+theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
        scale_x_datetime(breaks = date_breaks('1 day'), labels = date_format("%m-%d"))
```

---

### NRC Decision Tree 
maximum Tree

```{r, fig.height=10, fig.width=15}
full_nrc$diff = c(diff(full_nrc$price),0)
full_nrc$trend = ifelse(full_nrc$diff>0, 1, 0)
full_nrc$trend = factor(full_nrc$trend)
regTree = rpart(trend ~ anger+anticipation+disgust+fear+joy+negative+positive+sadness+surprise+trust, data = full_nrc, cp = 0.043)
rpart.plot(regTree, cex = 1.5)
prp(regTree)
pred_cart = predict(regTree,newdata = full_nrc[,5:15], type = 'class')
confusionMatrix(pred_cart, full_nrc$trend)
```